---
title: 'Examples'
description: 'Examples for common scenarios'
---

<CardGroup cols={2}>
  <Card
    title="Basic"
    href="https://github.com/empirical-run/empirical/tree/main/examples/basic"
  >
    Uses an entity extraction use-case to check for valid JSON outputs.
  </Card>
    <Card
    title="Tool calling"
    href="https://github.com/empirical-run/empirical/tree/main/examples/tool_calls"
  >
    Uses an LLM to grade the output responses and ensure that they do not
    contain "as a AI language model" in them.
  </Card>
  <Card
    title="Spider (TypeScript)"
    href="https://github.com/empirical-run/empirical/tree/main/examples/spider-using-ts"
  >
    Runs a subset of the [Spider dataset](https://github.com/taoyds/spider) to demo text-to-SQL and relevant scorer functions in TypeScript.
  </Card>
    <Card
    title="Spider (Python)"
    href="https://github.com/empirical-run/empirical/tree/main/examples/spider"
  >
    Runs a subset of the [Spider dataset](https://github.com/taoyds/spider) to demo text-to-SQL and relevant scorer functions in Python.
  </Card>
  <Card
    title="RAG"
    href="https://github.com/empirical-run/empirical/tree/main/examples/rag"
  >
    Tests a Retrieval-augmented Generation application built with LlamaIndex, scored on
    metrics from Ragas.
  </Card>
    <Card
    title="OpenAI Assistants"
    href="https://github.com/empirical-run/empirical/tree/main/examples/assistants"
  >
    Runs Empirical on an OpenAI Assistant.
  </Card>
  <Card
    title="HumanEval"
    href="https://github.com/empirical-run/empirical/tree/main/examples/humaneval"
  >
    Uses a custom Python scoring function to run the HumanEval benchmark, which is
    a popular dataset for code generation tasks.
  </Card>

  <Card
    title="Chat bot with LLM scorer"
    href="https://github.com/empirical-run/empirical/tree/main/examples/chatbot"
  >
    Uses an LLM to grade the output responses and ensure that they do not
    contain "as a AI language model" in them.
  </Card>
</CardGroup>
