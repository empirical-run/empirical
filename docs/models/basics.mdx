---
title: 'Basics'
description: 'Choose model providers to test with'
---

Empirical tests how different models and model configurations work for your
application. Choose the models and configurations by defining the configuration
for model providers.

Empirical supports two types of model providers:

- `model`: API calls to off-the-shelf LLMs, like OpenAI's GPT4
- `py-script`: Custom models or applications defined by a Python module

To configure a custom model with Python, see [the Python guide](./custom).

The rest of this doc focuses on the `model` type.

## Run configuration for LLMs

Specify the `provider`, `model` and `prompt` keys to configure this. See below
for supported providers.

Use placeholders in the prompt (like `{{user_name}}`) to replace the placeholder with the
actual value from the sample inputs. See [dataset](../dataset/basics) to learn more about
sample inputs.

You can configure as many model providers as you like. These models will be shown in a 
side-by-side comparison view in the web reporter.

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "prompt": "Hey I'm {{user_name}}"
  },
]
```

## Supported providers

| Provider | Description |
|----------|-------------|
| `openai` | All chat models are supported. Requires `OPENAI_API_KEY` environment variable. |
| `anthropic` | Claude 3 models are supported. Requires `ANTHROPIC_API_KEY` environment variable. |
| `mistral` | All chat models are supported. Requires `MISTRAL_API_KEY` environment variable. |
| `google` | Gemini Pro models are supported. Requires `GEMINI_API_KEY` environment variable. |
| `fireworks` | Models hosted on Fireworks (e.g. `dbrx-instruct`) are supported. Requires `FIREWORKS_API_KEY` environment variable. |

### Environment variables

API calls to model providers require API keys, which are stored as environment variables. The CLI can work with:

- Existing environment variables (using `process.env`)
- Environment variables defined in `.env` or `.env.local` files, in the current working directory
  - For .env files that are located elsewhere, you can pass the `--env-file` flag

```sh
npx @empiricalrun/cli --env-file <PATH_TO_ENV_FILE>
```

### Model configuration parameters

To override parameters like `temperature` or `max_tokens`, you can pass `config` alongwith the provider
information. All OpenAI configurations (see their [API reference](https://platform.openai.com/docs/api-reference/chat/create))
are supported, except for a few [limitations](#limitations).

For non-OpenAI models, we coerce these parameters to the most appropriate target parameter (e.g. `stop` in OpenAI
becomes `stop_sequences` for Anthropic.) For additional parameters, see [passthrough](#passthrough).

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "prompt": "Hey I'm {{user_name}}",
    "config": {
      "temperature": 0.1
    }
  },
]
```

#### Passthrough

If your models rely on other configuration parameters, you can use the `passthrough` feature, which
passes the parameter value as-is to the model.

For example, Mistral models support a `safePrompt` parameter for [guardrailing](https://docs.mistral.ai/platform/guardrailing/).

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "mistral",
    "model": "mistral-tiny",
    "prompt": "Hey I'm {{user_name}}",
    "config": {
      "passthrough": {
        "safePrompt": true
      }
    }
  },
]
```

#### Limitations

We don't support these parameters today: `logit_bias`, `tools`, `tool_choice`, `user`, `stream`. If you need these, please file a [feature request](https://github.com/empirical-run/empirical/issues/new).
