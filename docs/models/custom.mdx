---
title: 'Custom model or app'
description: 'Specify your application or model with a Python script'
---

Using a Python function as the entry-point, you can define a **custom model** to test
with Empirical. This method can be also used to **test an application**, which 
does pre or post-processing around the LLM call or chains multiple LLM calls together.

## Run configuration

A minimal configuration looks like:

```json
"runs": [
  {
    "type": "py-script",
    "path": "rag.py"
  }
]
```

<ParamField body="type" type="string" required>
  Should be "py-script"
</ParamField>
<ParamField body="path" type="string" required>
  Specify path to the Python file, which must have a function `def execute` (see [file structure](#file-structure))
</ParamField>
<ParamField body="parameters" type="object">
  JSON object of parameters passed to the `execute` method to customize script behavior
</ParamField>
<ParamField body="name" type="string">
  A custom name or label for this run (auto-generated if not specified)
</ParamField>

## File structure 

The Python file is expected to have a method called `execute` with the following
signature:

```python rag.py
def execute(inputs, parameters):
    # call the model and other processing here    
    # ...
    return {
        "value": output, # string
        "metadata": {
            "key": value # string
        }
    }
```

- **Arguments**
  - inputs: dict of key-value pairs with [sample inputs](../dataset/basics)
  - parameters: dict of key-value pairs with the run parameters
- **Returns**: an output dict with
  - value (string): The response from the model/application
  - metadata (dict): Custom key-value pairs that are passed on to the scorer and
    web reporter

## Example

The [RAG example](https://github.com/empirical-run/empirical/tree/main/examples/rag)
uses this model provider to test a RAG application. The `metadata` field is used to capture the retrieved context.
