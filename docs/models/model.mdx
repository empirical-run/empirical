---
title: 'Configuration'
description: 'Choose model providers to test with'
---

## Run configuration for LLMs

To test an LLM, specify the following properties in the configuration

<ParamField body="type" type="string" required>
  Should be "model"
</ParamField>
<ParamField body="provider" type="string" required>
  Name of the inference provider (e.g. `openai`, or other [supported providers](./providers))
</ParamField>
<ParamField body="prompt" type="string" required>
  [Prompt](#prompt) sent to the model, with optional [placeholders](#placeholders)
</ParamField>
<ParamField body="parameters" type="object">
  JSON object of [parameters](#model-parameters) to customize the model behavior
</ParamField>
<ParamField body="name" type="string">
  A custom name or label for this run (auto-generated if not specified)
</ParamField>

You can configure as many model providers as you like. These models will be shown in a 
side-by-side comparison view in the web reporter.

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "prompt": "Hey I'm {{user_name}}"
  },
  {
    "type": "model",
    "provider": "fireworks",
    "model": "llama-v3-8b-instruct",
    "prompt": "Hey I'm {{user_name}}"
  }
]
```

### Prompt
The prompt serves as the initial input provided to the model to generate a response. 
This property accepts either a string or a JSON chat format.

<CodeGroup>

```json Prompt as string
{
  "runs": [
    {
      "prompt": "You are an SQLite expert who can convert natural language questions to SQL queries. What is the SQL query for this question: {{question}}"
    }
  ]
}
```

```json Prompt as JSON
{
  "runs": [
    {
      "prompt": [
        {
          "role": "system",
          "content": "You are an SQLite expert who can convert natural language questions to SQL queries."
        },
        {
          "role": "user",
          "content": "{{question}}"
        }
      ]
    }
  ]
}
```

</CodeGroup>

#### Prompt format: string

String prompts are wrapped in `user` role message before sending to the model.

The [basic example](https://github.com/empirical-run/empirical/tree/main/examples/basic) uses this prompt 
format to test extraction of named entities from natural language text.

#### Prompt format: JSON

The JSON chat format allows for a sequence of messages comprising the conversation so far. 
Each message object has two required fields:
- `role`: Role of the messenger (either `system`, `user` or `assistant`)
- `content`: The content of the message

The [Text-to-SQL example](https://github.com/empirical-run/empirical/tree/main/examples/spider)
uses this prompt format to test conversion of natural language questions to SQL queries.

## Placeholders

Define placeholders in the prompt with Handlebars syntax (like `{{user_name}}`) to inject values
from the dataset sample. These placeholders will be replaced with the corresponding input value
during execution.

See [dataset](../dataset/basics) to learn more about sample inputs.


## Model parameters

To override parameters like `temperature` or `max_tokens`, you can pass `parameters` along with the provider
configuration. All OpenAI parameters (see their [API reference](https://platform.openai.com/docs/api-reference/chat/create))
are supported, except for a few [limitations](#limitations).

For non-OpenAI models, we coerce these parameters to the most appropriate target parameter (e.g. `stop` in OpenAI
becomes `stop_sequences` for Anthropic.)

You can add other parameters or override this behavior with [passthrough](#passthrough).

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "prompt": "Hey I'm {{user_name}}",
    "parameters": {
      "temperature": 0.1
    }
  }
]
```

### Tool calling

Hosted models support tool calling. You can use the `tools` parameter to specify
functions that are provided to the model.

See [output object](./output) to see how the model response object stores tool calls.

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "openai",
    "model": "gpt-4o",
    "prompt": "Add these numbers {{numberOne}} and {{numberTwo}}",
    "parameters": {
      "tools": [
        {
          "type": "function",
          "function": {
            "name": "add_numbers",
            "description": "Helper function to add numbers",
            "parameters": {
              "type": "object",
              "properties": {
                "number_a": {
                  "type": "number",
                  "description": "The first number"
                },
                "number_b": {
                  "type": "number",
                  "description": "The second number"
                }
              }
            }
          }
        }
      ]
    }
  }
]
```

### Passthrough

If your models rely on other parameters, you can still specify them in the configuration. These
parameters will be passed as-is to the model.

For example, Mistral models support a `safePrompt` parameter for [guard railing](https://docs.mistral.ai/platform/guardrailing/).

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "mistral",
    "model": "mistral-tiny",
    "prompt": "Hey I'm {{user_name}}",
    "parameters": {
      "temperature": 0.1,
      "safePrompt": true
    }
  }
]
```

### Request timeout

You can set the timeout duration in milliseconds under model parameters in the `empiricalrc.json` file. This might be required for prompt completions that are expected to take more time, for example while running models like Claude Opus. If no specific value is assigned, the default timeout duration of 30 seconds will be applied.

```json empiricalrc.json
"runs": [
  {
    "type": "model",
    "provider": "anthropic",
    "model": "claude-3-opus",
    "prompt": "Hey I'm {{user_name}}",
    "parameters": {
      "timeout": 10000
    }
  }
]
```
