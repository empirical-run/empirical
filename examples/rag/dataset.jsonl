{"question":"How can expert annotations, such as radiologist eye-gaze heatmaps, be integrated into CLIP models to improve performance in medical imaging analysis?","contexts":["(a)Histogram of Cosine Similarities Among Subgroups. The\nmodel exhibits high cosine similarity among embeddings within the\ntext (red) and image ( blue) modalities, regardless of the differences\namong subgroups. This underscores the model\u2019s challenge in capturing\nthe subtleties inherent in medical data.\n(b)Modality Gap. Despite the\nCLIP contrastive loss aiming at\nclosely aligning image and text em-\nbeddingswithinasharedspace, the\nmodalities remain segregated into\ndistinct regions.\nFigure1:AnalysisofCLIPEmbeddingsinMedicalImaging Thefigurepresentsembeddingsgenerated\nby a CLIP model, pretrained on an internet-scale dataset, applied to the Open-I dataset pairing X-rays with\ncorresponding radiology reports.\nmodel. In 1a, we investigate the embeddings generated by a CLIP model \u2013 initially pretrained on internet\ndata \u2013 using samples from the Open-I dataset(Demner-Fushman et al., 2016), which includes X-rays and\ncorrespondingradiologyreports. Wecategorizethesamplesintosubgroupsbasedontheprimaryabnormality\nidentified in each report, such as \u2018normal\u2019, cardiomegaly, atelectasis and opacity. A histogram of the cosine\nsimilarities between embeddings from different groups indicates a high degree of similarity, with values\napproaching 1. This could lead to potential challenges in downstream zero-shot inference tasks, which rely\non the spatial segregation of embeddings from different groups (Radford et al., 2021). Typically, continual\npretraining on medically relevant data is employed to enhance the model\u2019s ability to differentiate between\nvarious abnormalities.\nRecentstudies(Liangetal.,2022;Goeletal.,2022;Ohetal.,2023;Zhangetal.,2024;Tschannenetal.,2023)\nhave identified a \u201cmodality gap\u201d in multi-modal contrastive representation learning, where the embeddings\nfrom different modalities (e.g., images and text) fall in distinct regions in the shared embedding space. This\nseparation, which arises from factors such as initial model weights and the objectives of contrastive learning\n(Liang et al., 2022; Zhang et al., 2024), leads to the \u201ccone effect\u201d where embeddings of each modality are\nrestricted to a narrow region of the embedding hypersphere. In 1b, we illustrate this within the medical\ndomain with a 2D UMAP(McInnes et al., 2018) projection of image and text embeddings. This example\nhighlights how embeddings from the same modality but different semantic groups, such as X-ray images of\nvarying abnormalities, cluster closely together. This makes it difficult for a model to distinguish between\nsemantically different images, undermining its performance in medical image analysis.\nWe investigate the potential of integrating expert annotations, specifically radiologist eye-gaze heatmaps,\nto alleviate these issues. Processing the eye-gaze data from radiologists(Karargyris et al., 2021) provides us\nwith heatmaps indicative of the radiologist\u2019s attention across different regions of the X-ray images. This\nheatmap reflects areas of clinical interest aligned with details present in radiology reports. We posit that\nthis could help capture nuanced visual cues in the X-rays and therefore pairing it with reports can enrich\nthe CLIP training data with high-quality positive pairs. Due to the scarcity of such expert annotated data,\nwe employ the mixup strategy, a data augmentation technique which has been effective in both supervised\n(Zhang et al., 2017; Verma et al., 2019; Han et al., 2022) and contrastive learning(Verma et al., 2021; Oh\net al., 2023), to create additional synthetic samples.\nWe present eCLIP (expert-annotated CLIP), an adaptation of the CLIP model that incorporates expert\neye-gaze heatmaps, without modifying the CLIP model\u2019s core architecture. The operational workflow of\neCLIP is depicted in Fig. 2. Our contributions are as follows:\n2"],"ground_truth":"Expert annotations, such as radiologist eye-gaze heatmaps, can be integrated into CLIP models to improve performance in medical imaging analysis by providing heatmaps indicative of the radiologist's attention across different regions of X-ray images. These heatmaps reflect areas of clinical interest aligned with details present in radiology reports, helping capture nuanced visual cues in X-rays and enriching the CLIP training data with high-quality positive pairs. Additionally, the mixup strategy, a data augmentation technique, can be employed to create additional synthetic samples.","evolution_type":"simple","metadata":[{"page_label":"2","file_name":"2403.10153.pdf","file_path":"\/Users\/saikatmitra\/repo\/empirical\/examples\/rag\/arxiv-papers\/2403.10153.pdf","file_type":"application\/pdf","file_size":1500693,"creation_date":"2024-03-22","last_modified_date":"2024-03-22"}],"episode_done":true}
{"question":"How does eCLIP use eye-gaze heatmaps to improve medical imaging analysis?","contexts":["Improving Medical Multi-modal Contrastive Learning\nwith Expert Annotations\nYogesh Kumar yogesh.kumar@aalto.fi\nDepartment of Computer Science\nAalto University, Finland\nPekka Marttinen pekka.marttinen@aalto.fi\nDepartment of Computer Science\nAalto University, Finland\nAbstract\nWe introduce eCLIP, an enhanced version of the CLIP model that integrates expert anno-\ntations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive\nmulti-modal medical imaging analysis, notably data scarcity and the \u201cmodality gap\u201d \u2013 a\nsignificantdisparitybetweenimageandtextembeddingsthatdiminishesthequalityofrepre-\nsentations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor\nand leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus\nboosting the model\u2019s learning effectiveness. eCLIP is designed to be generally applicable to\nany variant of CLIP without requiring any modifications of the core architecture. Through\ndetailed evaluations across several tasks, including zero-shot inference, linear probing, cross-\nmodal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a\nfrozen Large Language Model, eCLIP showcases consistent improvements in embedding\nquality. The outcomes reveal enhanced alignment and uniformity, affirming eCLIP\u2019s capa-\nbility to harness high-quality annotations for enriched multi-modal analysis in the medical\nimaging domain.\n1 Introduction\nPretraining foundation models on multi-modal data \u2013 particularly leveraging the relationships between text\nand images \u2013 has proven to be a robust strategy for generating versatile embeddings (Radford et al., 2021;\nJia et al., 2021). These embeddings enhance the efficacy in several downstream tasks, from image generation\nto advanced vision-language integration (Liu et al., 2023; Ramesh et al., 2021; Singh et al., 2022). Central\nto this approach is the employment of a contrastive learning (CL) loss objective (Oord et al., 2018; Chen\net al., 2020; Zbontar et al., 2021), where models are trained to align positive pairs (e.g., an image and its\ncorresponding caption) while diversifying negative ones. A significant hurdle in this approach is the necessity\nof vast datasets, often comprising several millions of data points, for competitive results. Models such as\nCLIP (Radford et al., 2021) have been trained on internet-scale datasets, estimated to encompass hundreds\nof millions of image-text pairs (Cherti et al., 2023; Xu et al., 2023a). Acquiring datasets of this magnitude\nposes substantial challenges in specialized fields that require expert knowledge for data collection, processing\nand annotation. The medical imaging domain exemplifies these difficulties, where acquiring even a single\ndata point, such as a chest X-ray, involves complex processes requiring expertise and significant resources.\nMoreover, the procurement of such data for machine learning research is further complicated by ethical\nconsiderations, patient privacy concerns and the need for extensive de-identification procedures.\nThis has led to the prevalent use of foundation models, initialized with weights from models trained on\nextensive internet-scale datasets, for tasks in the medical domain (Zhang et al., 2022; Wang et al., 2022;\nHuang et al., 2021; Krishnan et al., 2022). However, the areas of interest within medical images are often\nnuanced and require expert knowledge to interpret, rendering them indistinguishable to a general-purpose\n1arXiv:2403.10153v1  [cs.CV]  15 Mar 2024"],"ground_truth":"eCLIP integrates expert annotations in the form of radiologist eye-gaze heatmaps to tackle key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the 'modality gap'. It leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture.","evolution_type":"multi_context","metadata":[{"page_label":"1","file_name":"2403.10153.pdf","file_path":"\/Users\/saikatmitra\/repo\/empirical\/examples\/rag\/arxiv-papers\/2403.10153.pdf","file_type":"application\/pdf","file_size":1500693,"creation_date":"2024-03-22","last_modified_date":"2024-03-22"}],"episode_done":true}
